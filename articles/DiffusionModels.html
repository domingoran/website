<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Domingo Ranieri - Article - DiffusionModel</title>
    <link rel="stylesheet" href="../styles/diffusionmodel.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css"
        integrity="sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xLiPY/NS5R+E6ztJQ=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700;800&display=swap"
        rel="stylesheet">
    <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
              tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
              }
            });
          </script>

    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

</head>

<body class="main-content">
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
    <div class="main-title">
        <h2> <span id="title">DIFFUSION MODELS</span><span id="log" class="bg-text">RanDom </span></h2>
    </div>
    <div class="image">
        <img src="../img/opera.jpg" />
        <p>Theatre d’Opera Spatial, Jason Allen</p>
    </div>
    <div id="column1">

        <div class="date">
            <h3>6 May 2023</h3>
        </div>
        <div class="lectureTime">
            <h3>Estimated reading time: 15 min</h3>
        </div>
        <h1>Index</h1>
        <ul>
            <li><a href="#intro" target="_parent">Introduction</a></li>
            <li><a href="#section1" target="_parent">Destroy to reconstruct</a></li>
            <ul class="customIndent">
                <li><a href="#subsection1" target="_parent">Forwad diffusion</a></li>
                <li><a href="#subsection2" target="_parent">Reverse diffusion</a></li>
            </ul>

            <li><a href="#section2" target="_parent">
                    Sculpting pictures</a></li>
            <li><a href="#section3" target="_parent">
                    Image Entropy</a></li>
            <li><a href="#section4" target="_parent">
                    Rules are made to be broken</a></li>
            <li><a href="#section5" target="_parent">
                    Conclusion</a></li>
        </ul>
        <br />
        <h1 id="intro"> Introduction</h1>
        <p>

            If you had an internet connection in the last year,
            you have seen many images generated by AI.
            Above we have $Theatre$ $d’Opera$ $Spatial$ by Jason Allen, which won the
            Colorado State Fair Fine Arts Competition.
            Of all the Artificial intelligence generative models,
            the most used nowadays are all based on Diffusion Model.
            <br />
            The main ideas behind this model are expressed and developed in a couple of papers. <a
                href="https://arxiv.org/pdf/1503.03585.pdf" target="_blank"> <b>$Deep$ $Unsupervised$ $Learning$ $using$
                    $Nonequilibrium$ $Thermodynamics$</b></a>
            and <a href="https://arxiv.org/pdf/2006.11239.pdf" target="_blank"> <b>$Denoising$ $Diffusion$
                    $Probabilistic$
                    $Models.$</b></a>

            The mathematical derivation is not very difficult, but I do not want to be too technical. However, I leave
            some links to go deeper. If you have any doubts <a href="contact.html" target="_blank"> <b>contact
                    me</b></a>.
            <br /> <br />
        </p>


        <!-- $$ {J(\theta) =\frac{1}{2m} [\sum^m_{i=1}(h_\theta(x^{(i)}) - y^{(i)})2 + \lambda\sum^n_{j=1}\theta^2_j}
        \tag{1}$$ -->
        <h2 id="section1"> Destroy to reconstruct</h2>
        <p>
            The essential idea was inspired by non-equilibrium statistical physics.
            Given a data distribution, its structure is systematically and slowly destroyed through
            an iterative forward diffusion process. The second step is to perform a reverse diffusion process that
            restores structure in data, yielding a highly flexible and tractable generative model of the data.<br />
            An image is valid data distribution that can be modified by adding recursively normally distributed noise.
            When the
            process
            is repeated a lot of times, the original image becomes a pure noise distribution. If it is possible to
            perform
            the reverse operation, we could be able to start from pure noise and generate an image. As you can imagine
            the
            forward process is not as difficult to implement as the reverse one. <br />
            In general, destruction is the easiest part if you want to create something meaningful, although it could
            be
            difficult.<br /><br />
        </p>
        <h3 id="subsection1">Forward diffusion</h3>
        <p>

            As anticipated, the forward diffusion process, can be computed directly.
            We define $x_0$ the original image and
            $x_t$
            the same image after $t$ forward diffusion steps and $x_T$ is the final image which
            follows
            an isotropic Gaussian.<br>
            The Forward process is expressed as: $$q(x_t |x_{t-1})=N(x_t,\sqrt{1-\beta_t}x_{t-1},\beta_t I) \tag{1} $$
            $N$ is a normal distribution and the
            three terms on the parenthesis are respectively: the output, the mean and the variance.
            Then $$\beta_t \in [0,1]$$ is a term used to ensure that the variance and mean are bounded in a correct
            range of values.
            It can be used to set a schedule which changes the amount of noise added at each step.
            In the first paper, they used a linear schedule, while in the 2021 OpenAI paper: <a
                href="https://arxiv.org/pdf/2102.09672.pdf" target="_blank"> <b>$Improved$ $Denoising$ $Diffusion$
                    $Probabilistic$
                    $Models$</b></a>, is introduced a cosine
            schedule.
            In the first case, the lost of information is too fast and the
            pure noise state is reached many steps before the end of the forward process.
            Thus applying the first expression, we can obtain the next image starting from the previous.
            However, we can rewrite this expression in a closed form to find $x_t$ starting from
            $x_0$ in a single step: $$q(x_t |x_0)=\sqrt{\bar{\alpha_t}}x_0
            +\sqrt{1-\bar{\alpha_t}}\epsilon $$
            and
            $$\epsilon \sim N(0,1)$$
            is a normally distributed noise.
            <br /><br />
        </p>

        <h3 id="subsection2">Reverse diffusion</h3>
        <p>

            In the reverse diffusion process we want to remove at each step a layer of noise going from
            $x_t$ to $x_{t-1}$.
            This process is possible if we can find the noise to remove.
            However it is not computable directly, so a neural network is trained to find it.
            <!-- The reverse process can be written as: $$p(x_{t-1} |x_{t})=
            N(x_{t-1},\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t))\tag{2}
            $$ -->
            The target distribution is:
            $$q(x_{t-1} |x_{t})=
            N(x_{t-1},\tilde{\mu_{\theta}}(x_t,x_0),\tilde{\beta_t}I)$$
            while the approximated distribution is:
            $$p_{\theta}(x_{t-1} |x_{t})=
            N(x_{t-1},\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t))$$
            We know that find the noise to remove is equivalent to find the mean and variance of
            the normal distribution used to generate the noise.
            In the first paper, they fixed the variance using a schedule and trained a neural network to
            predict the mean of the distribution.
            In the OpenAI paper they trained a second neural network to learn the variance, improving the results.
            <!-- At first we define a Loss function, in this case we use a Negative Log-Likelihood:
            $$Loss=-\log{p_{\theta}(x_0)}$$.
            It depends on each <i>x<sub>t</sub></i> thus it is intractable. Instead of optimizing it,
            we can optimize the Variational Lower Bound.
            If we optimize a computable lower bound, we indirectly optimize the intractable loss function indeed.
            $$-\log{p_{\theta}(x_0)} \le -\log{p_{\theta}(x_0)}+D_{KL}(q(x_{1:T}|x_0)||p_{\theta}(x_{1:T}|x_0))
            \tag{3}$$
            where <i>D<sub>KL</sub></i> is a measure of how similar two distribution are.
            $$D_{KL}(p||q)=\int_x{p(x)\log{\frac{p(x)}{q(x)}}dx}$$
            The R.H.S. in expression (3) is equivalent to:
            $$ D_{KL}(q(x_T|x_0)||p(x_T))+\sum_{t=2}^T
            D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))-\log(p_{\theta}(x_0|x_1))$$
            The first term:
            $$D_{KL}(q(x_T|x_0)||p(x_T))$$
            depends on a forward process and a pure noise factor,
            it can be neglected cause it has no parameter to learn.
            The last term known as <i>Reconstruction term</i>:
            $$-\log{p_{\theta}}(x_0|x_1)$$
            is the reconstruction loss of the last denoising step. It can be
            ignored during training because it can be approximated using the same neural network in  -->
            The core expression used to perform the training in the first paper is the sum of three terms.
            <!-- $$ D_{KL}(q(x_T|x_0)||p(x_T))+\sum_{t=2}^T
            D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))-\log(p_{\theta}(x_0|x_1))$$ -->
            The first term:
            $$D_{KL}(q(x_T|x_0)||p(x_T))$$
            depends on a forward process and a pure noise factor,
            we can neglect it cause it has no parameter to learn.
            The last term:
            $$-\log{p_{\theta}}(x_0|x_1)$$
            is related with the last denoising step. We can
            ignore it during training without changing the performance of the neural network.
            While the second term:
            $$\sum_{t=2}^T
            D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))$$
            is the most important. $D_{KL}$ measures how similar two distribution are. Minimising this term, we
            train the neural
            network to find the approximated distribution $p_{\theta}(x_{t-1} |x_{t})$.<br>
            We note the presence of $x_0$ in the first distribution. It is the original image that we want to
            reconstruct, it works
            as a hint for the reverse process.

            <br /><br />

        </p>
        <h2 id="section2">Sculping pictures</h2>
        <p>
            As we have seen, this model generates images in a completely different way than usual.
            There are two main differences with classical drawing: it does not start from a white canvas, and it does
            not
            add layers over the base.
            In fact, the generation starts from a pure noise <i>"canvas"</i> and removes layers of noise to
            leave the desired image.<br>
            This procedure remembers me of Michelangelo's words:
            $$ I\ mean\ sculpture\ that\ is\ done\ by\ force\ of\ removal,$$
            $$that\ which\ is\ done\ by\ way\ of\ placing\ is\ similar\ to\
            painting.$$
            in this sense, we are sculpting pictures.
            Obviously, it is not a perfect analogy, but why this proces seems so strange to us?


            <br /><br />
        </p>
        <h2 id="section3">Image Entropy</h2>
        <p>
            Each image is a collection of pixels and each pixel has a color that can be encoded in a series of numbers.
            If all the pixels of the image have the same colour, the entropy of the picture is minimum and equal to
            zero,
            while a pure noise image has the maximum level of entropy. Here we are talking about Shannon's entropy which
            is
            deeply related to the concept of information. In particular, a low level of entropy is associated with a
            high
            level
            of information and vice versa. But what kind of information has an image with all the pixels of the same
            colour?
            A high level of information means that knowing the value of a particular pixel, you can guess with high
            the probability of the near pixels values.
            In a monochrome image, if you know the value of one pixel, you know all the others with probability equal to
            one, so
            it
            has
            the maximum level of information. In a pure noise image, the probability to guess a pixel value is equal to
            (all
            the possible values)<sup>-1</sup>, we have no information.
            In general, in a picture, there are zone whit similar colours, so around a certain pixel, we have a
            high
            probability to
            find close values. The level of information is not so low.
            <br /><br />
        </p>
        <h2 id="section4">Rules are made to be broken</h2>
        <p>
            As we all know, the 2<sup>nd</sup> law of thermodynamics states that the entropy in the Universe is always
            increasing.
            Different artist's operas could have different levels of entropy (such as Rotcho and Pollock)
            but all of them have something in common: the drawing process increases the entropy of the image.
            To be more precise, the entropy of the final picture is greater or equal to the white starting canvas.
            In the diffusion model, we have the opposite process: the final image has a lower (or equal to) entropy than
            the
            starting one.
            It seems so strange because it is an inverse diffusion process which is as particular as the smoke that
            comes back
            to
            the fire. This process does not break the 2<sup>nd</sup> law of thermodynamics. To decrease the entropy, we
            trained a
            neural
            network and many processes are necessary to perform this task, increasing the entropy of the
            Universe.
            <br /><br />
        </p>
        <h2 id="section5">Conclusion</h2>
        <p>
            Diffusion models are a peculiar example of a generative model.
            Here we only grasp the surface, but many other aspects and ideas are implemented
            in the technologies of nowadays. Here are some links to go deeper if you are interested.
            <br /><br />
        <ul>
            <li><a href="https://arxiv.org/pdf/1503.03585.pdf" target="_blank"> $Deep$ $Unsupervised$ $Learning$
                    $using$
                    $Nonequilibrium$ $Thermodynamics$, 2015</a></li>
            <li><a href="https://arxiv.org/pdf/2006.11239.pdf" target="_blank"> $Denoising$ $Diffusion$
                    $Probabilistic$
                    $Models$, 2020 </a></li>
            <li><a href="https://arxiv.org/pdf/2102.09672.pdf" target="_blank">$Improved$ $Denoising$ $Diffusion$
                    $Probabilistic$
                    $Models$, OpenAI, 2021</a></li>
            <li><a href="https://arxiv.org/pdf/2105.05233.pdf" target="_parent">$Diffusion$ $Models$ $Beat$ $GANs$ $on$
                    $Image$ $Synthesis$, OpenAI, 2021</a></li>
            <li><a href="https://www.youtube.com/watch?v=HoKDTa5jHvg" target="_parent">Video explanation with
                    mathematical derivation.</a></li>
        </ul>

        <br /><br />
        </p>
        <!-- <h2 id="Appendix">Appendix</h3>
            <p>
                <br />
                We want to prove that:
                $$q(x_t |x_0)=\sqrt{\bar{\alpha_t}}x_0
                +\sqrt{1-\bar{\alpha_t}}\epsilon $$
                At first we define:
                $$\alpha_t = 1- \beta_t$$
                and
                $$\bar{\alpha_t}=\prod_{s=1}^t\alpha_s$$
                Using the reparameterization trick:
                $$N(\mu,\sigma^2)=\mu+\sigma \cdot \epsilon$$
                $$\epsilon \sim N(0,1)$$
                the expression
                $$q(x_t |x_{t-1})=N(x_t,\sqrt{1-\beta_t}x_{t-1},\beta_t I)$$
                becames:
                $$q(x_t |x_{t-1})=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t} \epsilon$$
                $$=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t} \epsilon$$
                We can iterate the process expressing $x_{t-1}$ using $x_{t-2}$ so it
                becames:
                $$q(x_t |x_{t-2})=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}} \epsilon$$
                This passage is not difficult but a little bit long. We use again the reparameterization trick and the
                fact that the distribution of a variable that is the sum of variables normall distributed is normal
                distributed itself.
                If we repete this step until $x_0$ is reached we get:
                $$q(x_t |x_0)=\sqrt{\prod_{i=0}^t\alpha_i}x_0+\sqrt{1-\prod_{i=0}^t\alpha_i} \epsilon$$
                $$=\sqrt{\bar{\alpha_t}}x_0
                +\sqrt{1-\bar{\alpha_t}}\epsilon$$
                and
                $$q(x_t |x_0)=N(x_t,\sqrt{\bar{\alpha_t}}x_0,1-\bar{\alpha_t}I) $$

            </p> -->

    </div>
    <div class="cookies-eu-banner hidden">
        <div class="content ">
            <h1>Accept Cookies </h1>
            <p>By clicking <b style="color: #036564;">ACCEPT</b> you agree to the storing of
                cookies on your
                device to
                enhance a better experience.</p>
            <div class="btns">
                <button class="btn cancel"><b>Reject</b></button>
                <button class="btn accept"><b>Accept</b></button>
            </div>
        </div>
    </div>
    <footer class="footer">
        <ul class="footer_options">
            <li class="footer_item"><a href="">Copyright</a> </li>
            <li class="footer_item"><a href="">Terms and conditions</a></li>
            <li class="footer_item"><a href="">Cookie policy</a></li>
            <li class="footer_item"><a class="cookie-settings" href="">Change cookies settings</a></li>
        </ul>
    </footer>
    <script src="../scripts/main.js"></script>
    <script src="../scripts/logo.js"></script>
</body>



</html>